{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "\n",
    "class BaseHOLinear(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 bias=True, \n",
    "                 nblock=2,\n",
    "                 order=2,\n",
    "                 set_order_weights=None, \n",
    "                 softmax_order_weights=False):\n",
    "        '''\n",
    "        in_channels: int, number of input channels\n",
    "        out_channels: int, number of output channels\n",
    "        bias: bool, whether to use bias\n",
    "        nblock: int, number of blocks to split the input channels into\n",
    "        order: int, order of the operator\n",
    "        set_order_weights: list or None, list of order weights\n",
    "        softmax_order_weights: bool, whether to apply softmax to the order weights\n",
    "        '''\n",
    "        super(BaseHOLinear, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.nblock = nblock\n",
    "        self.order = order\n",
    "        assert self.order <= self.nblock, \"order should be less than or equal to nblock\"\n",
    "\n",
    "        self.dim_split = [in_channels // nblock * i for i in range(nblock)] + [in_channels]\n",
    "\n",
    "        if set_order_weights is None:\n",
    "            self.order_weights = torch.nn.Parameter(torch.Tensor([0] + [1.0] + [0] * (order-1)))\n",
    "        else: \n",
    "            # fix the order weights\n",
    "            assert len(set_order_weights) == order + 1, \"set_order_weights should have length order + 1\"\n",
    "            self.register_buffer('order_weights', torch.Tensor(set_order_weights))\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_channels, in_channels))\n",
    "        if bias: self.bias = torch.nn.Parameter(torch.Tensor(out_channels))\n",
    "        else: self.bias = None\n",
    "\n",
    "        self.softmax_order_weights = softmax_order_weights\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #TODO: initialization can be modified according to the specific task\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None: torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.order == 1:\n",
    "            return F.linear(x, self.weight, self.bias)\n",
    "        \n",
    "        ips = []\n",
    "        for i in range(self.nblock):\n",
    "            indices = torch.arange(self.dim_split[i], self.dim_split[i+1], dtype=torch.long).to(x.device)\n",
    "            ips.append(F.linear(\n",
    "                torch.index_select(x, -1, indices), \n",
    "                torch.index_select(self.weight, -1, indices)))\n",
    "        # ips: [nblock, :, out_channels]\n",
    "        if self.softmax_order_weights: order_weights = F.softmax(self.order_weights, dim=0)\n",
    "        else: order_weights = self.order_weights\n",
    "        result = torch.sum(torch.stack(ips), 0) * order_weights[1] + order_weights[0]\n",
    "        for cur_order in range(2, self.order+1):\n",
    "            for combination in itertools.combinations(ips, cur_order):\n",
    "                ips_comb = torch.stack(combination)\n",
    "                result += torch.prod(ips_comb, 0) * order_weights[cur_order]\n",
    "        if self.bias is not None: return result + self.bias\n",
    "        else: return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HODot(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                nblock=2,\n",
    "                order=2,\n",
    "                set_order_weights=None, \n",
    "                softmax_order_weights=False):\n",
    "        super(HODot, self).__init__()\n",
    "        self.nblock = nblock\n",
    "        self.order = order\n",
    "        assert self.order <= self.nblock, \"order should be less than or equal to nblock\"\n",
    "\n",
    "        if set_order_weights is None:\n",
    "            self.order_weights = torch.nn.Parameter(torch.Tensor([0] + [1.0] + [0] * (order-1)))\n",
    "        else: \n",
    "            # fix the order weights\n",
    "            assert len(set_order_weights) == order + 1, \"set_order_weights should have length order + 1\"\n",
    "            self.register_buffer('order_weights', torch.Tensor(set_order_weights))\n",
    "\n",
    "        self.softmax_order_weights = softmax_order_weights\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        assert x.shape == y.shape, \"x and y must be of the same shape\" \n",
    "\n",
    "        if self.order == 1:\n",
    "            return torch.sum(x * y, -1)\n",
    "\n",
    "        dim = x.shape[-1]\n",
    "        dim_split = [dim // self.nblock * i for i in range(self.nblock)] + [dim]\n",
    "        \n",
    "        ips = []\n",
    "        for i in range(self.nblock):\n",
    "            indices = torch.arange(dim_split[i], dim_split[i+1], dtype=torch.long).to(x.device)\n",
    "            ips.append(torch.sum(\n",
    "                torch.index_select(x, -1, indices) * torch.index_select(y, -1, indices), -1))\n",
    "        # ips: [nblock, :]\n",
    "        if self.softmax_order_weights: order_weights = F.softmax(self.order_weights, dim=0)\n",
    "        else: order_weights = self.order_weights\n",
    "        result = torch.sum(torch.stack(ips), 0) * order_weights[1] + order_weights[0]\n",
    "        for cur_order in range(2, self.order+1):\n",
    "            for combination in itertools.combinations(ips, cur_order):\n",
    "                ips_comb = torch.stack(combination)\n",
    "                result += torch.prod(ips_comb, 0) * order_weights[cur_order]\n",
    "        else: return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 4, 5)\n",
    "y = torch.rand(3, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotlayer = HODot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7407, 0.4689, 1.1121, 1.2297],\n",
       "        [1.6793, 2.1328, 0.6844, 1.2036],\n",
       "        [2.1045, 1.9425, 1.4902, 1.2142]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotlayer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7407, 0.4689, 1.1121, 1.2297],\n",
       "        [1.6793, 2.1328, 0.6844, 1.2036],\n",
       "        [2.1045, 1.9425, 1.4902, 1.2142]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransConvLayer(nn.Module):\n",
    "    '''\n",
    "    transformer with fast attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 out_channels,\n",
    "                 num_heads,\n",
    "                 use_weight=True):\n",
    "        super().__init__()\n",
    "        self.Wk = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        self.Wq = nn.Linear(in_channels, out_channels * num_heads)\n",
    "        if use_weight:\n",
    "            self.Wv = nn.Linear(in_channels, out_channels * num_heads)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, output_attn=False):\n",
    "        # feature transformation\n",
    "        qs = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        ks = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            vs = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            vs = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2)  # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2)  # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs)  # [N, H, D]\n",
    "        attention_num += N * vs\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(\n",
    "            attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer  # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks).mean(dim=-1)  # [N, N]\n",
    "            normalizer = attention_normalizer.squeeze(dim=-1).mean(dim=-1, keepdims=True)  # [N,1]\n",
    "            attention = attention / normalizer\n",
    "\n",
    "        final_output = attn_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attention\n",
    "        else:\n",
    "            return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransConvLayer(nn.Module):\n",
    "    '''\n",
    "    transformer with fast attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 out_channels,\n",
    "                 num_heads,\n",
    "                 use_weight=True):\n",
    "        super().__init__()\n",
    "        self.Wk = BaseHOLinear(in_channels, out_channels * num_heads, nblock=4, order=4, bias=False)\n",
    "        self.Wq = BaseHOLinear(in_channels, out_channels * num_heads, nblock=4, order=4, bias=False)\n",
    "        if use_weight:\n",
    "            self.Wv = BaseHOLinear(in_channels, out_channels * num_heads, nblock=4, order=4, bias=False)\n",
    "\n",
    "        self.AttDot = HODot(nblock=4, order=2, softmax_order_weights=True)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, output_attn=False):\n",
    "        # feature transformation\n",
    "        qs = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        ks = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            vs = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            vs = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2)  # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2)  # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        # attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs)  # [N, H, D]\n",
    "        def compute_attnum(qs, kvs):\n",
    "            n, h, m = qs.shape\n",
    "            _, _, d = kvs.shape\n",
    "            expand_qs = qs.expand(d, n, h, m)\n",
    "            expand_kvs = kvs.permute(2, 0, 1).expand(n, d, h, m).transpose(0, 1)\n",
    "            # return torch.sum(expand_qs * expand_kvs, 3).permute(1, 2, 0)\n",
    "            return self.AttDot(expand_qs, expand_kvs).permute(1, 2, 0)\n",
    "        attention_num = compute_attnum(qs, kvs)\n",
    "        attention_num += N * vs\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        # attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "        # attention_normalizer = torch.sum(qs * ks_sum.expand_as(qs), 2)  # [N, H]\n",
    "        attention_normalizer = self.AttDot(qs, ks_sum.expand_as(qs))  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(\n",
    "            attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer  # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks).mean(dim=-1)  # [N, N]\n",
    "            normalizer = attention_normalizer.squeeze(dim=-1).mean(dim=-1, keepdims=True)  # [N,1]\n",
    "            attention = attention / normalizer\n",
    "\n",
    "        final_output = attn_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attention\n",
    "        else:\n",
    "            return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransConvLayerV2(nn.Module):\n",
    "    '''\n",
    "    transformer with fast attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 out_channels,\n",
    "                 num_heads,\n",
    "                 use_weight=True):\n",
    "        super().__init__()\n",
    "        self.Wk = BaseHOLinear(in_channels, out_channels * num_heads, nblock=4, order=4, bias=False)\n",
    "        self.Wq = BaseHOLinear(in_channels, out_channels * num_heads, nblock=4, order=4, bias=False)\n",
    "        if use_weight:\n",
    "            self.Wv = BaseHOLinear(in_channels, out_channels * num_heads, nblock=4, order=4, bias=False)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.use_weight = use_weight\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.Wk.reset_parameters()\n",
    "        self.Wq.reset_parameters()\n",
    "        if self.use_weight:\n",
    "            self.Wv.reset_parameters()\n",
    "\n",
    "    def forward(self, query_input, source_input, output_attn=False):\n",
    "        # feature transformation\n",
    "        qs = self.Wq(query_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        ks = self.Wk(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        if self.use_weight:\n",
    "            vs = self.Wv(source_input).reshape(-1, self.num_heads, self.out_channels)\n",
    "        else:\n",
    "            vs = source_input.reshape(-1, 1, self.out_channels)\n",
    "\n",
    "        # normalize input\n",
    "        qs = qs / torch.norm(qs, p=2)  # [N, H, M]\n",
    "        ks = ks / torch.norm(ks, p=2)  # [L, H, M]\n",
    "        N = qs.shape[0]\n",
    "\n",
    "        # numerator\n",
    "        kvs = torch.einsum(\"lhm,lhd->hmd\", ks, vs)\n",
    "        attention_num = torch.einsum(\"nhm,hmd->nhd\", qs, kvs)  # [N, H, D]\n",
    "        attention_num += N * vs\n",
    "\n",
    "        # denominator\n",
    "        all_ones = torch.ones([ks.shape[0]]).to(ks.device)\n",
    "        ks_sum = torch.einsum(\"lhm,l->hm\", ks, all_ones)\n",
    "        attention_normalizer = torch.einsum(\"nhm,hm->nh\", qs, ks_sum)  # [N, H]\n",
    "\n",
    "        # attentive aggregated results\n",
    "        attention_normalizer = torch.unsqueeze(\n",
    "            attention_normalizer, len(attention_normalizer.shape))  # [N, H, 1]\n",
    "        attention_normalizer += torch.ones_like(attention_normalizer) * N\n",
    "        attn_output = attention_num / attention_normalizer  # [N, H, D]\n",
    "\n",
    "        # compute attention for visualization if needed\n",
    "        if output_attn:\n",
    "            attention = torch.einsum(\"nhm,lhm->nlh\", qs, ks).mean(dim=-1)  # [N, N]\n",
    "            normalizer = attention_normalizer.squeeze(dim=-1).mean(dim=-1, keepdims=True)  # [N,1]\n",
    "            attention = attention / normalizer\n",
    "\n",
    "        final_output = attn_output.mean(dim=1)\n",
    "\n",
    "        if output_attn:\n",
    "            return final_output, attention\n",
    "        else:\n",
    "            return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "in_feat = 20\n",
    "q = torch.rand(n, in_feat)\n",
    "k = torch.rand(n, in_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = MyTransConvLayer(in_feat, in_feat+2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 22])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(q, k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = MyTransConvLayerV2(in_feat, in_feat+2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
